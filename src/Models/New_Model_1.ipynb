{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas básicas\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch y modelos\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm  # Biblioteca para modelos preentrenados avanzados\n",
    "\n",
    "# Scikit-learn para métricas y división de datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros generales\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "SEED = 123\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# Fijar las semillas para reproducibilidad\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al directorio de imágenes\n",
    "images_dir = '/ruta/a/tu/directorio/arcgis-survey-images-new'\n",
    "\n",
    "# Obtener las clases y asignar etiquetas numéricas\n",
    "class_names = sorted([d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))])\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "print(f\"Clases encontradas: {class_names}\")\n",
    "\n",
    "# Crear listas para rutas de imágenes y etiquetas\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(images_dir, class_name)\n",
    "    for fname in os.listdir(class_dir):\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            image_paths.append(os.path.join(class_dir, fname))\n",
    "            labels.append(class_to_idx[class_name])\n",
    "\n",
    "# Crear DataFrame\n",
    "data_df = pd.DataFrame({\n",
    "    'filepath': image_paths,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Mostrar distribución de clases\n",
    "print(\"Distribución de clases:\")\n",
    "print(data_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    data_df,\n",
    "    test_size=0.2,\n",
    "    stratify=data_df['label'],\n",
    "    random_state=SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar OpenCV\n",
    "import cv2\n",
    "\n",
    "# Transformación personalizada\n",
    "class CustomTransform:\n",
    "    def __call__(self, image):\n",
    "        # Convertir PIL Image a NumPy array\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        # Convertir a escala de grises\n",
    "        gray_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Aplicar filtro de Sobel\n",
    "        sobelx = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "        sobely = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "        sobel = cv2.magnitude(sobelx, sobely)\n",
    "        sobel = cv2.normalize(sobel, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        sobel = sobel.astype(np.uint8)\n",
    "        \n",
    "        # Convertir de vuelta a RGB\n",
    "        sobel_rgb = cv2.cvtColor(sobel, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Convertir a PIL Image\n",
    "        image_filtered = Image.fromarray(sobel_rgb)\n",
    "        \n",
    "        return image_filtered\n",
    "\n",
    "# Transformaciones para entrenamiento y validación\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        CustomTransform(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485], [0.229])  # Normalización para imágenes en escala de grises\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        CustomTransform(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485], [0.229])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.filepaths = df['filepath'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Cargar imagen\n",
    "        image = Image.open(self.filepaths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datasets\n",
    "train_dataset = LeafDataset(train_df, transform=data_transforms['train'])\n",
    "val_dataset = LeafDataset(val_df, transform=data_transforms['val'])\n",
    "\n",
    "# Crear dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo Swin Transformer preentrenado\n",
    "model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "\n",
    "# Modificar la última capa para el número de clases\n",
    "model.head = nn.Linear(model.head.in_features, len(class_names))\n",
    "\n",
    "# Mover el modelo al dispositivo\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelar capas inferiores\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layers' in name and int(name.split('.')[1]) < 2:  # Congelar las primeras 2 capas\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir criterio de pérdida\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definir optimizador con weight decay para regularización\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Scheduler para ajustar la tasa de aprendizaje\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    early_stopping_patience = 5\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Cada época tiene una fase de entrenamiento y validación\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Modo entrenamiento\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Modo evaluación\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterar sobre datos\n",
    "            for inputs, labels in tqdm(dataloader):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Habilitar gradientes solo en entrenamiento\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backpropagation y optimización solo en entrenamiento\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Estadísticas\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Profundizar si hay mejora en la precisión\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "        # Early Stopping\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(f'Mejor precisión en validación: {best_acc:.4f}')\n",
    "    # Cargar los mejores pesos\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model, history = train_model(model, criterion, optimizer, scheduler, num_epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.softmax(dim=1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    return all_preds, np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener predicciones en el conjunto de validación\n",
    "preds, val_labels = evaluate_model(model, val_loader)\n",
    "\n",
    "# Obtener las clases predichas\n",
    "pred_classes = np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nReporte de clasificación del modelo:\\n\")\n",
    "print(classification_report(val_labels, pred_classes, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(val_labels, pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, xticklabels=class_names, yticklabels=class_names, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de pérdida y precisión\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Pérdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Entrenamiento')\n",
    "plt.plot(history['val_loss'], label='Validación')\n",
    "plt.legend()\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Pérdida durante el Entrenamiento')\n",
    "\n",
    "# Precisión\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Entrenamiento')\n",
    "plt.plot(history['val_acc'], label='Validación')\n",
    "plt.legend()\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Precisión durante el Entrenamiento')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
