{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3289 files belonging to 5 classes.\n",
      "Using 2632 files for training.\n",
      "Found 3289 files belonging to 5 classes.\n",
      "Using 657 files for validation.\n",
      "Nueva distribución de clases después del sobremuestreo:\n",
      "Chinche salivosa: 414\n",
      "Clororis: 413\n",
      "Hoja sana: 415\n",
      "Roya naranja: 415\n",
      "Roya purpura: 383\n",
      "Pesos de clase: {0: 0.7963691376701967, 1: 1.6097859327217126, 2: 0.7927710843373494, 3: 0.6588235294117647, 4: 2.9082872928176795}\n",
      "Epoch 1/50\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 488ms/step - accuracy: 0.2842 - loss: 4.0439 - val_accuracy: 0.2496 - val_loss: 2.5527 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 478ms/step - accuracy: 0.4415 - loss: 2.0104 - val_accuracy: 0.2481 - val_loss: 2.1427 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 474ms/step - accuracy: 0.5017 - loss: 1.5412 - val_accuracy: 0.1963 - val_loss: 1.6614 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 470ms/step - accuracy: 0.5284 - loss: 1.4210 - val_accuracy: 0.3212 - val_loss: 1.8348 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 470ms/step - accuracy: 0.5424 - loss: 1.3009 - val_accuracy: 0.2374 - val_loss: 3.0052 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 469ms/step - accuracy: 0.5578 - loss: 1.2177 - val_accuracy: 0.1766 - val_loss: 2.5877 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460ms/step - accuracy: 0.5851 - loss: 1.1629"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Parámetros de la red\n",
    "EPOCHS = 50\n",
    "IMAGE_SIZE = (128, 128)\n",
    "INPUT_SHAPE = (128, 128, 3)\n",
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 250\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Directorio de las imágenes\n",
    "images_dir = '../arcgis-survey-images'\n",
    "\n",
    "# Cargar datasets de entrenamiento y validación\n",
    "train_ds = image_dataset_from_directory(\n",
    "    images_dir,\n",
    "    labels=\"inferred\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_ds = image_dataset_from_directory(\n",
    "    images_dir,\n",
    "    labels=\"inferred\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Función para contar ejemplos en un dataset\n",
    "def count_examples(dataset):\n",
    "    return dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "# Manejo del Desbalanceo de Clases mediante Sobremuestreo\n",
    "def oversample_dataset(dataset, majority_size):\n",
    "    return dataset.repeat().take(majority_size)\n",
    "\n",
    "# Filtrar cada clase\n",
    "def filter_class(dataset, class_label):\n",
    "    return dataset.filter(lambda x, y: tf.reduce_any(tf.equal(y, class_label)))\n",
    "\n",
    "# Contar ejemplos por clase\n",
    "def get_class_sizes(dataset, num_classes):\n",
    "    sizes = []\n",
    "    for i in range(num_classes):\n",
    "        class_ds = filter_class(dataset, i)\n",
    "        size = count_examples(class_ds)\n",
    "        sizes.append(size)\n",
    "    return sizes\n",
    "\n",
    "# Obtener el tamaño de la clase mayoritaria\n",
    "train_class_sizes = get_class_sizes(train_ds, num_classes)\n",
    "majority_class_size = max(train_class_sizes)\n",
    "\n",
    "# Sobremuestrear cada clase\n",
    "oversampled_datasets = []\n",
    "for i in range(num_classes):\n",
    "    class_ds = filter_class(train_ds, i)\n",
    "    oversampled_ds = oversample_dataset(class_ds, majority_class_size)\n",
    "    oversampled_datasets.append(oversampled_ds)\n",
    "\n",
    "# Concatenar los datasets sobremuestreados\n",
    "oversampled_train_ds = oversampled_datasets[0]\n",
    "for ds in oversampled_datasets[1:]:\n",
    "    oversampled_train_ds = oversampled_train_ds.concatenate(ds)\n",
    "\n",
    "# Aplicar optimización de cache y prefetch\n",
    "oversampled_train_ds = oversampled_train_ds.cache().shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Verificar la nueva distribución de clases\n",
    "print(\"Nueva distribución de clases después del sobremuestreo:\")\n",
    "for i, class_size in enumerate(get_class_sizes(oversampled_train_ds, num_classes)):\n",
    "    print(f\"{class_names[i]}: {class_size}\")\n",
    "\n",
    "# Aumento de Datos (Data Augmentation)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "# Preprocesamiento de Imágenes\n",
    "def preprocess_image(image, label):\n",
    "    # Normalizar imágenes\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "# Aplicar aumento de datos y preprocesamiento solo al conjunto de entrenamiento\n",
    "def prepare_train_ds(ds):\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.cache().shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def prepare_val_ds(ds):\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "oversampled_train_ds = prepare_train_ds(oversampled_train_ds)\n",
    "validation_ds = prepare_val_ds(validation_ds)\n",
    "\n",
    "# Obtener las etiquetas de entrenamiento para calcular los pesos de clase\n",
    "y_train = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "\n",
    "# Calcular los pesos de clase\n",
    "class_weights_values = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights_values))\n",
    "print(\"Pesos de clase:\", class_weights_dict)\n",
    "\n",
    "# Cargar el modelo base (EfficientNetB0) con pesos preentrenados\n",
    "base_model = EfficientNetB0(input_shape=INPUT_SHAPE,\n",
    "                            include_top=False,\n",
    "                            weights='imagenet')\n",
    "\n",
    "# Congelar las primeras capas del modelo base\n",
    "for layer in base_model.layers[:100]:  # Ajusta este número según sea necesario\n",
    "    layer.trainable = False\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Usar softmax para SparseCategoricalCrossentropy\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=LEARNING_RATE,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks para mejorar el entrenamiento\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
    "]\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    oversampled_train_ds,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Gráfica de la pérdida y precisión\n",
    "def plot_history(history):\n",
    "    metrics = history.history\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Pérdida\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.epoch, metrics['loss'], label='Pérdida de entrenamiento')\n",
    "    plt.plot(history.epoch, metrics['val_loss'], label='Pérdida de validación')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, max(metrics['loss']) * 1.1])\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.xlabel('Época')\n",
    "    plt.title('Pérdida durante el Entrenamiento')\n",
    "    \n",
    "    # Precisión\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.epoch, metrics['accuracy'], label='Precisión de entrenamiento')\n",
    "    plt.plot(history.epoch, metrics['val_accuracy'], label='Precisión de validación')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.xlabel('Época')\n",
    "    plt.title('Precisión durante el Entrenamiento')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluación en el conjunto de test\n",
    "test_ds = validation_ds.shard(num_shards=2, index=1)\n",
    "test_results = model.evaluate(test_ds, return_dict=True)\n",
    "print(\"\\nResultados de evaluación en test set:\")\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Predicciones y matriz de confusión\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, xticklabels=class_names, yticklabels=class_names, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# Fine-Tuning del modelo base\n",
    "# Descongelar todas las capas para fine-tuning\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompilar el modelo con una tasa de aprendizaje más baja\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento adicional para fine-tuning\n",
    "fine_tune_epochs = 20\n",
    "total_epochs = EPOCHS + fine_tune_epochs\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    oversampled_train_ds,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Gráfica de la pérdida y precisión después del fine-tuning\n",
    "plot_history(history_finetune)\n",
    "\n",
    "# Evaluación final en el conjunto de test\n",
    "final_test_results = model.evaluate(test_ds, return_dict=True)\n",
    "print(\"\\nResultados de evaluación final en test set después del fine-tuning:\")\n",
    "for metric, value in final_test_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Guardar el modelo final\n",
    "model.save('final_model.keras')\n",
    "print(\"\\nModelo final guardado como 'final_model.keras'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
