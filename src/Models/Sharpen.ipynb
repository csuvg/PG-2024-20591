{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4308 files belonging to 5 classes.\n",
      "Using 3447 files for training.\n",
      "Found 4308 files belonging to 5 classes.\n",
      "Using 861 files for validation.\n",
      "Nueva distribución de clases después del sobremuestreo:\n",
      "Chinche salivosa: 539\n",
      "Clororis: 533\n",
      "Hoja sana: 540\n",
      "Roya naranja: 540\n",
      "Roya purpura: 540\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345127eb7e064301baef77a916202d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entrenando:   0%|                                              | 0/100 [00:00<?, ?época/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 256ms/step - accuracy: 0.7785 - loss: 1.3963 - val_accuracy: 0.5528 - val_loss: 7.5917\n",
      "Epoch 2/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 235ms/step - accuracy: 0.9355 - loss: 0.2802 - val_accuracy: 0.6446 - val_loss: 4.2091\n",
      "Epoch 3/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 239ms/step - accuracy: 0.9706 - loss: 0.1429 - val_accuracy: 0.7921 - val_loss: 3.1822\n",
      "Epoch 4/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 253ms/step - accuracy: 0.9806 - loss: 0.1067 - val_accuracy: 0.7851 - val_loss: 3.4550\n",
      "Epoch 5/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 240ms/step - accuracy: 0.9827 - loss: 0.1087 - val_accuracy: 0.5168 - val_loss: 8.0962\n",
      "Epoch 6/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 234ms/step - accuracy: 0.9810 - loss: 0.1094 - val_accuracy: 0.8223 - val_loss: 1.3983\n",
      "Epoch 7/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 233ms/step - accuracy: 0.9925 - loss: 0.0557 - val_accuracy: 0.7967 - val_loss: 2.4132\n",
      "Epoch 8/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 233ms/step - accuracy: 0.9920 - loss: 0.0626 - val_accuracy: 0.7154 - val_loss: 4.1241\n",
      "Epoch 9/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 233ms/step - accuracy: 0.9883 - loss: 0.0883 - val_accuracy: 0.8269 - val_loss: 1.6986\n",
      "Epoch 10/100\n",
      "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 235ms/step - accuracy: 0.9960 - loss: 0.0349 - val_accuracy: 0.8699 - val_loss: 0.9408\n",
      "Epoch 11/100\n",
      "\u001b[1m309/540\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 228ms/step - accuracy: 0.9931 - loss: 0.0454"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"Plague Classification Model with ArcGIS Data\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import logging\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Parámetros de la red\n",
    "EPOCHS = 100\n",
    "IMAGE_SIZE = (128, 128)\n",
    "INPUT_SHAPE = (128, 128, 3)\n",
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 250\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Directorio de las imágenes\n",
    "images_dir = '../last_data'\n",
    "\n",
    "\n",
    "# Cargar dataset de imágenes\n",
    "train_ds = image_dataset_from_directory(\n",
    "    images_dir,\n",
    "    labels=\"inferred\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_ds = image_dataset_from_directory(\n",
    "    images_dir,\n",
    "    labels=\"inferred\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Obtener los nombres de las clases antes de mapear\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "# Definir la función para aplicar el filtro Sharpen\n",
    "def apply_sharpen_filter(image, label):\n",
    "    # Definir el kernel de Sharpen\n",
    "    sharpen_kernel = tf.constant([[0, -1, 0],\n",
    "                                  [-1, 5, -1],\n",
    "                                  [0, -1, 0]], dtype=tf.float32)\n",
    "    sharpen_kernel = sharpen_kernel[:, :, tf.newaxis, tf.newaxis]  # [3, 3, 1, 1]\n",
    "    sharpen_kernel = tf.tile(sharpen_kernel, [1, 1, 3, 1])  # [3, 3, 3, 1]\n",
    "\n",
    "    # Asegurarse de que la imagen es de tipo float32\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # Aplicar la convolución depthwise\n",
    "    sharpened_image = tf.nn.depthwise_conv2d(image, sharpen_kernel, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "    # Clip y conversión de tipo\n",
    "    sharpened_image = tf.clip_by_value(sharpened_image, 0, 255)\n",
    "    sharpened_image = tf.cast(sharpened_image, tf.uint8)\n",
    "\n",
    "    return sharpened_image, label\n",
    "\n",
    "# Aplicar el filtro Sharpen a los datasets\n",
    "train_ds = train_ds.map(apply_sharpen_filter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_ds = validation_ds.map(apply_sharpen_filter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Paso 1: Filtrar el dataset por cada clase usando tf.reduce_any para obtener un tensor booleano escalar\n",
    "chinche_salivosa_ds = train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 0)))\n",
    "clororis_ds = train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 1)))\n",
    "hoja_sana_ds = train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 2)))\n",
    "roya_naranja_ds = train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 3)))\n",
    "roya_purpura_ds = train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 4)))\n",
    "\n",
    "# Paso 2: Contar ejemplos manualmente\n",
    "def count_examples(dataset):\n",
    "    return dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "# Obtener el tamaño de la clase mayoritaria\n",
    "chinche_salivosa_size = count_examples(chinche_salivosa_ds)\n",
    "clororis_size = count_examples(clororis_ds)\n",
    "hoja_sana_size = count_examples(hoja_sana_ds)\n",
    "roya_naranja_size = count_examples(roya_naranja_ds)\n",
    "roya_purpura_size = count_examples(roya_purpura_ds)\n",
    "\n",
    "majority_class_size = max(\n",
    "    chinche_salivosa_size,\n",
    "    clororis_size,\n",
    "    hoja_sana_size,\n",
    "    roya_naranja_size,\n",
    "    roya_purpura_size\n",
    ")\n",
    "\n",
    "# Paso 3: Sobremuestrear las clases minoritarias\n",
    "chinche_salivosa_ds = chinche_salivosa_ds.repeat().take(majority_class_size)\n",
    "clororis_ds = clororis_ds.repeat().take(majority_class_size)\n",
    "hoja_sana_ds = hoja_sana_ds.repeat().take(majority_class_size)\n",
    "roya_naranja_ds = roya_naranja_ds.repeat().take(majority_class_size)\n",
    "roya_purpura_ds = roya_purpura_ds.repeat().take(majority_class_size)\n",
    "\n",
    "# Paso 4: Concatenar los datasets sobremuestreados\n",
    "oversampled_train_ds = chinche_salivosa_ds.concatenate(clororis_ds)\n",
    "oversampled_train_ds = oversampled_train_ds.concatenate(hoja_sana_ds)\n",
    "oversampled_train_ds = oversampled_train_ds.concatenate(roya_naranja_ds)\n",
    "oversampled_train_ds = oversampled_train_ds.concatenate(roya_purpura_ds)\n",
    "\n",
    "# Paso 5: Aplicar las operaciones de optimización de cache y prefetch\n",
    "oversampled_train_ds = oversampled_train_ds.cache().shuffle(BUFFER_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Paso 6: Verificar la nueva distribución de clases\n",
    "# Filtrar el dataset final por clase para contar los ejemplos nuevamente\n",
    "oversampled_chinche_salivosa_ds = oversampled_train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 0)))\n",
    "oversampled_clororis_ds = oversampled_train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 1)))\n",
    "oversampled_hoja_sana_ds = oversampled_train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 2)))\n",
    "oversampled_roya_naranja_ds = oversampled_train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 3)))\n",
    "oversampled_roya_purpura_ds = oversampled_train_ds.filter(lambda x, y: tf.reduce_any(tf.equal(y, 4)))\n",
    "\n",
    "# Contar la cantidad de ejemplos en cada clase\n",
    "oversampled_chinche_salivosa_size = count_examples(oversampled_chinche_salivosa_ds)\n",
    "oversampled_clororis_size = count_examples(oversampled_clororis_ds)\n",
    "oversampled_hoja_sana_size = count_examples(oversampled_hoja_sana_ds)\n",
    "oversampled_roya_naranja_size = count_examples(oversampled_roya_naranja_ds)\n",
    "oversampled_roya_purpura_size = count_examples(oversampled_roya_purpura_ds)\n",
    "\n",
    "# Mostrar la distribución de clases final\n",
    "print(f\"Nueva distribución de clases después del sobremuestreo:\")\n",
    "print(f\"Chinche salivosa: {oversampled_chinche_salivosa_size}\")\n",
    "print(f\"Clororis: {oversampled_clororis_size}\")\n",
    "print(f\"Hoja sana: {oversampled_hoja_sana_size}\")\n",
    "print(f\"Roya naranja: {oversampled_roya_naranja_size}\")\n",
    "print(f\"Roya purpura: {oversampled_roya_purpura_size}\")\n",
    "\n",
    "# Cargar el modelo base (MobileNetV2)\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "base_model = MobileNetV2(input_shape=INPUT_SHAPE,\n",
    "                         include_top=False,\n",
    "                         weights='imagenet')\n",
    "\n",
    "# Ajuste de las capas del modelo base\n",
    "for layer in base_model.layers[:100]:  # Ajusta según sea necesario\n",
    "    layer.trainable = False\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.5),  # Aumentamos el Dropout a 0.5\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')  # Cambiamos la activación a 'softmax'\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=LEARNING_RATE,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Definir los callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Definir el callback TQDMCallback\n",
    "class TQDMCallback(Callback):\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.progbar = tqdm(total=epochs, desc='Entrenando', unit='época', ncols=90)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.progbar.update(1)\n",
    "        self.progbar.set_postfix({\n",
    "            'loss': f\"{logs.get('loss'):.4f}\",\n",
    "            'accuracy': f\"{logs.get('accuracy'):.4f}\",\n",
    "            'val_loss': f\"{logs.get('val_loss'):.4f}\",\n",
    "            'val_accuracy': f\"{logs.get('val_accuracy'):.4f}\"\n",
    "        })\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.progbar.close()\n",
    "\n",
    "# Entrenamiento del modelo con los callbacks\n",
    "history = model.fit(\n",
    "    oversampled_train_ds,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[TQDMCallback(EPOCHS), early_stopping, checkpoint],\n",
    ")\n",
    "\n",
    "# Guardar el modelo en formato .h5 y .keras\n",
    "model.save('final_model.keras')\n",
    "# model.save('final_model.h5')\n",
    "\n",
    "# Gráfica de la pérdida y precisión\n",
    "metrics = history.history\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, metrics['loss'], label='training')\n",
    "plt.plot(history.epoch, metrics['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, metrics['accuracy'], label='training')\n",
    "plt.plot(history.epoch, metrics['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "# Evaluación en el conjunto de test\n",
    "test_ds = validation_ds.shard(num_shards=2, index=1)\n",
    "test_results = model.evaluate(test_ds, return_dict=True)\n",
    "print(\"Resultados de evaluación en test set:\")\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "conf_matrix = tf.math.confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, xticklabels=class_names, yticklabels=class_names, annot=True, fmt='g')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta verdadera')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
